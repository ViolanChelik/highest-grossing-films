# parse_and_insert.py
import requests
from bs4 import BeautifulSoup
import psycopg2
import json

# 1. Fetch and parse the Wikipedia page
url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'html.parser')

# 2. Locate the correct table (assuming the first wikitable is the one we need)
table = soup.find('table', class_='wikitable')
films_data = []

# 3. Parse table rows (skipping header)
if table:
    rows = table.find_all('tr')
    for row in rows[1:]:
        cols = row.find_all(['th','td'])
        if len(cols) >= 5:  # ensuring there are enough columns
            title = cols[0].get_text(strip=True)
            release_year = cols[1].get_text(strip=True)
            director = cols[2].get_text(strip=True)
            box_office = cols[3].get_text(strip=True)
            country = cols[4].get_text(strip=True)
            
            # Data cleaning example: remove commas and currency symbols from box office
            # box_office_clean = box_office.replace('$','').replace(',','')
            # For this example, we'll keep it as text to preserve the formatting.
            
            try:
                release_year = int(release_year)
            except ValueError:
                release_year = None

            films_data.append({
                'title': title,
                'release_year': release_year,
                'director': director,
                'box_office': box_office,
                'country': country
            })

# 4. Insert data into PostgreSQL
conn = psycopg2.connect(
    dbname='dwv_a1_db_name', 
    user='georgi', 
    password='iakovlev', 
    host='localhost', 
    port='5432'
)
cur = conn.cursor()

# Create table if not exists
create_table_query = """
CREATE TABLE IF NOT EXISTS films (
    id SERIAL PRIMARY KEY,
    title TEXT NOT NULL,
    release_year INTEGER,
    director TEXT,
    box_office TEXT,
    country TEXT
);
"""
cur.execute(create_table_query)
conn.commit()

# Insert data
insert_query = """
INSERT INTO films (title, release_year, director, box_office, country)
VALUES (%s, %s, %s, %s, %s);
"""
for film in films_data:
    cur.execute(insert_query, (film['title'], film['release_year'], film['director'], film['box_office'], film['country']))
conn.commit()

# 5. Export data to JSON file
cur.execute("SELECT title, release_year, director, box_office, country FROM films;")
rows = cur.fetchall()
films_list = []
for row in rows:
    films_list.append({
        'title': row[0],
        'release_year': row[1],
        'director': row[2],
        'box_office': row[3],
        'country': row[4]
    })

with open('films.json', 'w', encoding='utf-8') as f:
    json.dump(films_list, f, ensure_ascii=False, indent=4)

# Close connection
cur.close()
conn.close()

print("Data extraction, insertion, and export completed.")
